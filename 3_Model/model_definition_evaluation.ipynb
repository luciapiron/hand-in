{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Import models you're considering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "After creating the OLS regression model that explains approximately 75% of the variance, it assumes linear relationships between predictors and Revenue. However since the revenue generation is likely influenced by other factors like lagged demand, seasonal components, product categories, and holiday effects. Therefore, we created a Neural Network, which can capture complex, non-linear relationships that linear regression cannot model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "First, we sorted the dataset chronologically to preserve temporal dependencies. A product group label was created from one-hot encoded product indicators and then we converted boolean variables to integers, and forward-filled missing values. A time-based split (70% train, 15% validation, 15% test) was applied and finally, we standardized all input features using StandardScaler to improve neural network convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (10896, 22)\n",
      "        Date  Holiday  NextDayHoliday  IsWeekend  Month  KielerWeek  \\\n",
      "0 2013-07-01        1               1          0      7           0   \n",
      "1 2013-07-01        1               1          0      7           0   \n",
      "2 2013-07-01        1               1          0      7           0   \n",
      "3 2013-07-01        1               1          0      7           0   \n",
      "4 2013-07-01        1               1          0      7           0   \n",
      "\n",
      "   IsNewYearsEve  IsHalloween  t      lag_1  ...  year_sin1  year_cos1  \\\n",
      "0              0            0  0  1269.2491  ...        0.0        1.0   \n",
      "1              0            0  0  1269.2491  ...        0.0        1.0   \n",
      "2              0            0  0  1269.2491  ...        0.0        1.0   \n",
      "3              0            0  0  1269.2491  ...        0.0        1.0   \n",
      "4              0            0  0  1269.2491  ...        0.0        1.0   \n",
      "\n",
      "   year_sin2  year_cos2    Revenue  Product_2  Product_3  Product_4  \\\n",
      "0        0.0        1.0  148.82835          0          0          0   \n",
      "1        0.0        1.0  535.85626          1          0          0   \n",
      "2        0.0        1.0  201.19843          0          1          0   \n",
      "3        0.0        1.0    0.00000          0          0          0   \n",
      "4        0.0        1.0  317.47586          0          0          0   \n",
      "\n",
      "   Product_5  Product_6  \n",
      "0          0          0  \n",
      "1          0          0  \n",
      "2          0          0  \n",
      "3          0          1  \n",
      "4          1          0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Product group distribution:\n",
      "Product_Group\n",
      "Product_1    1816\n",
      "Product_2    1816\n",
      "Product_3    1816\n",
      "Product_6    1816\n",
      "Product_5    1816\n",
      "Product_4    1816\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split sizes:\n",
      "Train: (7627, 20) (7627,)\n",
      "Val:   (1634, 20) (1634,)\n",
      "Test:  (1635, 20) (1635,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv('/workspaces/ml-project-template/final_dataset.csv')\n",
    "TARGET_COL = \"Revenue\"\n",
    "DATE_COL = \"Date\"\n",
    "\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "TRAIN_FRAC = 0.70\n",
    "VAL_FRAC = 0.15\n",
    "TEST_FRAC = 0.15\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded data shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# product groups\n",
    "\n",
    "product_cols = [c for c in df.columns if c.startswith(\"Product_\")]\n",
    "\n",
    "# If none of Product_2..Product_6 is True -> Product_1\n",
    "def infer_product_group(row):\n",
    "    for c in product_cols:\n",
    "        if bool(row[c]):\n",
    "            return c\n",
    "    return \"Product_1\"\n",
    "\n",
    "df[\"Product_Group\"] = df.apply(infer_product_group, axis=1)\n",
    "\n",
    "print(\"\\nProduct group distribution:\")\n",
    "print(df[\"Product_Group\"].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# Convert boolean columns to int\n",
    "bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "for c in bool_cols:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "\n",
    "# Features exclude target, date, product group label\n",
    "feature_cols = [c for c in df.columns if c not in [TARGET_COL, DATE_COL, \"Product_Group\"]]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "\n",
    "# splitting\n",
    "n = len(df)\n",
    "train_end = int(n * TRAIN_FRAC)\n",
    "val_end = int(n * (TRAIN_FRAC + VAL_FRAC))\n",
    "\n",
    "X_train, y_train = X.iloc[:train_end], y.iloc[:train_end]\n",
    "X_val, y_val = X.iloc[train_end:val_end], y.iloc[train_end:val_end]\n",
    "X_test, y_test = X.iloc[val_end:], y.iloc[val_end:]\n",
    "\n",
    "val_groups = df[\"Product_Group\"].iloc[train_end:val_end].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "#scaling the feautes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We tuned the neural network architecture manually by adjusting number of hidden layers and neurons, the dropout and learning rate and the batch size. Normalization was used to stabilize training. 20% dropout was used to reduce overfitting. We also chose to include EarlyStopping (patience = 12) in our model to prevent unnecessary training and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ml-project-template/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m2,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,825</span> (54.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,825\u001b[0m (54.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,441</span> (52.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,441\u001b[0m (52.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 50473.0078 - val_loss: 38337.4688\n",
      "Epoch 2/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19510.5117 - val_loss: 12013.6328\n",
      "Epoch 3/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7426.3403 - val_loss: 5864.0796\n",
      "Epoch 4/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5289.8110 - val_loss: 4021.8914\n",
      "Epoch 5/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4386.0137 - val_loss: 3838.3735\n",
      "Epoch 6/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4076.4854 - val_loss: 4523.5854\n",
      "Epoch 7/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3810.4397 - val_loss: 3705.8474\n",
      "Epoch 8/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3439.4719 - val_loss: 4121.7998\n",
      "Epoch 9/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3397.4817 - val_loss: 3526.8889\n",
      "Epoch 10/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3410.6670 - val_loss: 3863.6995\n",
      "Epoch 11/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3559.9746 - val_loss: 3492.4968\n",
      "Epoch 12/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3548.5305 - val_loss: 3404.8225\n",
      "Epoch 13/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3270.4233 - val_loss: 4244.6519\n",
      "Epoch 14/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3111.9707 - val_loss: 3500.3259\n",
      "Epoch 15/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3244.6941 - val_loss: 3852.6123\n",
      "Epoch 16/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2862.2043 - val_loss: 3604.0178\n",
      "Epoch 17/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3088.6453 - val_loss: 3866.9453\n",
      "Epoch 18/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3017.5208 - val_loss: 3741.6924\n",
      "Epoch 19/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2875.9622 - val_loss: 4106.7695\n",
      "Epoch 20/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2990.3887 - val_loss: 4041.1113\n",
      "Epoch 21/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2822.7883 - val_loss: 3648.0129\n",
      "Epoch 22/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2785.0032 - val_loss: 3559.0112\n",
      "Epoch 23/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2719.0632 - val_loss: 4267.3633\n",
      "Epoch 24/80\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2694.6296 - val_loss: 3967.6633\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, InputLayer, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1))  # regression output\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=12,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "     \n",
    "train_preds = model.predict(X_train_scaled).reshape(-1)\n",
    "val_preds = model.predict(X_val_scaled).reshape(-1)\n",
    "test_preds = model.predict(X_test_scaled).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Model performance was evaluated using Mean Absolute Percentage Error (MAPE), excluding zero-revenue observations. Rows with zero revenue were excluded from the calculation. MAPE provides an interpretable percentage-based error metric, allowing comparison across products and time periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAPE (excluding y=0):\n",
      "Training MAPE:   18.087%\n",
      "Validation MAPE: 21.961%\n",
      "Test MAPE:       36.391%\n",
      "\n",
      "Validation MAPE by Product Group (excluding y=0):\n",
      "Product_Group\n",
      "Product_1    30.845304\n",
      "Product_2    15.065353\n",
      "Product_3    27.906969\n",
      "Product_4    21.325962\n",
      "Product_5    14.638818\n",
      "Product_6          NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using your chosen metrics\n",
    "# Example for classification\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example for regression\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n",
    "\n",
    "# evaluating with MAPE\n",
    "def mape_excluding_zeros(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return mean_absolute_percentage_error(y_true[mask], y_pred[mask]) * 100\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "train_mape = mape_excluding_zeros(y_train, train_preds)\n",
    "val_mape = mape_excluding_zeros(y_val, val_preds)\n",
    "test_mape = mape_excluding_zeros(y_test, test_preds)\n",
    "\n",
    "print(\"\\nMAPE (excluding y=0):\")\n",
    "print(f\"Training MAPE:   {train_mape:.3f}%\")\n",
    "print(f\"Validation MAPE: {val_mape:.3f}%\")\n",
    "print(f\"Test MAPE:       {test_mape:.3f}%\")\n",
    "\n",
    "\n",
    "val_results = pd.DataFrame({\n",
    "    \"y_true\": y_val.reset_index(drop=True),\n",
    "    \"y_pred\": val_preds,\n",
    "    \"Product_Group\": val_groups\n",
    "})\n",
    "\n",
    "def group_mape(g):\n",
    "    g = g[g[\"y_true\"] != 0]\n",
    "    if len(g) == 0:\n",
    "        return np.nan\n",
    "    return mean_absolute_percentage_error(g[\"y_true\"], g[\"y_pred\"]) * 100\n",
    "\n",
    "group_mape_scores = val_results.groupby(\"Product_Group\").apply(group_mape).sort_index()\n",
    "\n",
    "print(\"\\nValidation MAPE by Product Group (excluding y=0):\")\n",
    "print(group_mape_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "The neural network achieved a training MAPE of 18.61%, validation MAPE of 24.19% and test MAPE of 37.76%. Compared to the OLS baseline (RMSE ≈ 76.82, R² ≈ 0.75), the neural network significantly reduced validation MSE (≈ 3049). This indicates that the neural network captures non-linear relationships between seasonal patterns, lag features, and product categories more effectively than the linear model. However, the increase in error from training to test suggests some degree of overfitting. Despite this, the neural network demonstrates better predictive performance compared to the baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
